name: e2e-test

on:
  push:
    branches:
      - main
  pull_request:

jobs:
  e2e-test:
    runs-on: ubuntu-latest

    services:
      postgres:
        image: postgres:13
        env:
          POSTGRES_USER: airflow
          POSTGRES_PASSWORD: airflow
          POSTGRES_DB: airflow
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U airflow -d airflow"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      redis:
        image: redis:latest
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      # 1. コード取得
      - uses: actions/checkout@v3

      # 2. 依存関係インストール
      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y docker-compose awscli google-cloud-cli

      # 3. Airflow 起動
      - name: Start Airflow with Docker Compose
        run: docker compose up -d

      # 4. Airflow DB 初期化 (修正ポイント: exec → run --rm)
      - name: Initialize Airflow DB
        run: docker compose run --rm airflow-scheduler airflow db init

      # 5. Webserver 起動待ち
      - name: Wait for Airflow webserver to be ready
        run: |
          for i in {1..15}; do
            if curl -s http://localhost:8080/health | grep '"status":"healthy"' ; then
              echo "Webserver is healthy"
              exit 0
            fi
            echo "⏳ Waiting for webserver... ($i/15)"
            sleep 10
          done
          echo "❌ Webserver did not start in time"
          exit 1

      # 6. DAG 実行
      - name: Trigger DAG run
        run: docker compose run --rm airflow-scheduler airflow dags trigger industry_metrics_from_input

      # 7. pytest 実行
      - name: Run pytest (check output files)
        run: |
          pip install -r requirements.txt
          pytest -v tests/test_forecast.py

      # 8. AWS CLI インストール済 → ファイル確認
      - name: Verify file uploaded to AWS S3
        run: |
          aws s3 ls s3://$AWS_S3_BUCKET/metrics/ --recursive

      # 9. GCP 認証 & ファイル確認
      - name: Authenticate to GCP
        run: echo "${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}" > ${HOME}/gcp-key.json

      - name: Verify file uploaded to GCS
        run: |
          gcloud auth activate-service-account --key-file=${HOME}/gcp-key.json
          gsutil ls gs://$GCS_BUCKET/metrics/

      # 10. BigQuery 確認
      - name: Verify data loaded into BigQuery
        run: |
          gcloud auth activate-service-account --key-file=${HOME}/gcp-key.json
          bq head -n 10 $BIGQUERY_DATASET.retail_metrics
