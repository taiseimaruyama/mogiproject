name: CI

on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]

jobs:
  build:
    runs-on: ubuntu-latest

    services:
      docker:
        image: docker:20.10-dind
        options: --privileged

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set up Docker Compose
        run: |
          sudo apt-get update
          sudo apt-get install -y docker-compose

      - name: Start Airflow with Docker Compose
        env:
          AIRFLOW_FERNET_KEY: ${{ secrets.AIRFLOW_FERNET_KEY }}
        run: |
          docker compose up -d

      - name: Wait for Airflow webserver to be healthy (with logs follow)
        run: |
          docker compose logs --follow airflow-webserver &
          LOG_PID=$!

          for i in {1..24}; do
            if curl -s http://localhost:8080/health; then
              echo "‚úÖ Airflow is healthy"
              kill $LOG_PID
              exit 0
            fi
            echo "‚è≥ Still waiting... ($i)"
            sleep 5
          done

          echo "‚ùå Airflow did not become healthy in time"
          kill $LOG_PID
          exit 1

      - name: Trigger Airflow DAG (with live logs)
        env:
          AIRFLOW_FERNET_KEY: ${{ secrets.AIRFLOW_FERNET_KEY }}
        run: |
          echo "üöÄ Triggering DAG industry_metrics_full_dag"
          docker compose exec -T airflow-webserver \
            airflow dags trigger industry_metrics_full_dag

          echo "üì° Streaming Airflow logs in real-time..."
          docker compose logs --follow airflow-scheduler &
          PID1=$!
          docker compose logs --follow airflow-webserver &
          PID2=$!
          docker compose logs --follow airflow-worker &
          PID3=$!

          for i in {1..10}; do   # 30ÁßíÈñìÈöî √ó 10Âõû = ÊúÄÂ§ß5ÂàÜ
            RUNS=$(docker compose exec -T airflow-webserver \
              airflow dags list-runs -d industry_metrics_full_dag --no-backfill --state success | tail -n +3 | wc -l)
            if [ "$RUNS" -gt 0 ]; then
              echo "‚úÖ DAG finished successfully"
              kill $PID1 $PID2 $PID3 || true
              exit 0
            fi

            FAILS=$(docker compose exec -T airflow-webserver \
              airflow dags list-runs -d industry_metrics_full_dag --no-backfill --state failed | tail -n +3 | wc -l)
            if [ "$FAILS" -gt 0 ]; then
              echo "‚ùå DAG failed"
              kill $PID1 $PID2 $PID3 || true
              exit 1
            fi

            echo "‚è≥ DAG still running... ($i)"
            sleep 30
          done

          echo "‚ùå DAG did not finish within 5 minutes"
          kill $PID1 $PID2 $PID3 || true
          exit 1

      - name: Test GCS connection
        run: |
          echo "Testing GCS connection..."
          # gsutil ls „Å®„ÅãÊé•Á∂öÁ¢∫Ë™ç„Ç≥„Éû„É≥„Éâ„ÇíÂÖ•„Çå„Çã

      - name: Test S3 connection
        run: |
          echo "Testing S3 connection..."
          aws s3 ls s3://domoproject || true

      - name: Test BigQuery connection
        run: |
          echo "Testing BigQuery connection..."
          # bq query --use_legacy_sql=false "SELECT 1"

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-northeast-1

      - name: Upload output files to S3
        if: always()
        run: |
          echo "üì§ Uploading files in ./output to S3..."
          ls -R output || echo "‚ö†Ô∏è output directory not found"
          if [ -d output ]; then
            aws s3 cp output/ s3://domoproject/ --recursive
          else
            echo "‚ö†Ô∏è No output directory found"
          fi
