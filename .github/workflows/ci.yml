name: CI

on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]

jobs:
  build:
    runs-on: ubuntu-latest
    env:
      AIRFLOW_FERNET_KEY: ${{ secrets.AIRFLOW_FERNET_KEY }}

    services:
      docker:
        image: docker:20.10-dind
        options: --privileged

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set up Docker Compose
        run: |
          sudo apt-get update
          sudo apt-get install -y docker-compose

      - name: Set up GCP credentials
        run: |
          mkdir -p keys
          echo '${{ secrets.GCP_CREDENTIALS }}' > keys/gcp.json

      - name: Initialize Airflow DB
        run: docker compose run --rm airflow-init

      - name: Start Airflow with Docker Compose
        run: docker compose up -d

      - name: Show running containers
        run: docker compose ps -a

      # üîΩ „Åì„Åì„Çí 3ÂàÜÂæÖÊ©ü „Åã„Çâ„ÄåÂÖ®„Ç≥„É≥„Éù„Éº„Éç„É≥„ÉàËµ∑ÂãïÂæÖ„Å°„Äç„Å´Â§âÊõ¥
      - name: Wait for Airflow components to be ready
        run: |
          echo "‚è≥ Waiting for Airflow webserver + scheduler + worker..."
          for i in {1..120}; do
            webserver_ok=false
            scheduler_ok=false
            worker_ok=false

            if curl -s http://localhost:8080/health | grep '"status": "healthy"' >/dev/null; then
              webserver_ok=true
            fi

            if docker compose exec -T airflow-scheduler airflow jobs check --job-type SchedulerJob >/dev/null 2>&1; then
              scheduler_ok=true
            fi

            if docker compose exec -T airflow-worker ps aux | grep "celery worker" | grep -v grep >/dev/null; then
              worker_ok=true
            fi

            if $webserver_ok && $scheduler_ok && $worker_ok; then
              echo "‚úÖ All Airflow components are ready"
              exit 0
            fi

            echo "‚è≥ Still waiting... ($i)"
            sleep 5
          done

          echo "‚ùå Airflow components did not become ready in time"
          echo "üìú Dumping worker logs for debugging..."
          docker compose logs --tail=200 airflow-worker
          exit 1

      - name: Trigger DAG
        run: |
          export RUN_ID="ci_run_$(date +%s)"
          echo "Using RUN_ID=$RUN_ID"
          docker compose exec -T airflow-webserver airflow dags trigger industry_metrics_full_dag --run-id "$RUN_ID"

      - name: Wait for DAG completion (30 min)
        run: |
          echo "Monitoring DAG Run (latest run)"
          for i in {1..360}; do
            latest=$(docker compose exec -T airflow-webserver \
              airflow dags list-runs -d industry_metrics_full_dag --no-backfill | grep "$RUN_ID" | head -n 1)
            echo "Latest run: $latest"

            if echo "$latest" | grep "success"; then
              echo "‚úÖ DAG finished successfully"
              exit 0
            elif echo "$latest" | grep "failed"; then
              echo "‚ùå DAG failed"
              exit 1
            fi

            echo "‚è≥ DAG still running..."
            sleep 5
          done

          echo "‚ùå DAG did not finish in 30 minutes"
          exit 1

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-northeast-1

      - name: Test GCS connection
        run: |
          echo "Testing GCS connection..."
          docker compose exec -T airflow-webserver gsutil ls gs://${{ secrets.GCS_BUCKET }} || true

      - name: Test S3 connection
        run: |
          echo "Testing S3 connection..."
          aws s3 ls s3://domoproject || true

      - name: Test BigQuery connection
        run: |
          echo "Testing BigQuery connection..."
          docker compose exec -T airflow-webserver \
            bq query --use_legacy_sql=false "SELECT 1" || true

      - name: Upload output files to S3
        if: always()
        run: |
          echo "üì§ Uploading files in ./output to S3..."
          ls -R output || echo "‚ö†Ô∏è output directory not found"
          if [ -d output ]; then
            aws s3 cp output/ s3://domoproject/ --recursive
          else
            echo "‚ö†Ô∏è No output directory found"
          fi

      - name: Stop containers
        if: always()
        run: docker compose down
