name: e2e-test

on:
  push:
    branches: [ "main" ]
  pull_request:

jobs:
  e2e-test:
    runs-on: ubuntu-latest

    services:
      postgres:
        image: postgres:13
        ports:
          - 5432:5432
        env:
          POSTGRES_USER: airflow
          POSTGRES_PASSWORD: airflow
          POSTGRES_DB: airflow
      redis:
        image: redis:latest
        ports:
          - 6379:6379

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.12"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Start Airflow with Docker Compose
        run: docker-compose up -d

      - name: Initialize Airflow DB
        run: |
          docker exec airflow_scheduler airflow db init

      - name: Wait for Airflow webserver to be ready
        run: |
          for i in {1..20}; do
            if docker logs airflow_webserver 2>&1 | grep -q "Listening at:"; then
              echo "✅ Webserver is ready"
              exit 0
            fi
            echo "⏳ Waiting for webserver... ($i/20)"
            sleep 15
          done
          echo "❌ Webserver did not start in time. Dumping logs..."
          docker logs airflow_webserver
          exit 1

      - name: Trigger DAG run
        run: |
          docker exec airflow_webserver \
            airflow dags trigger -d industry_metrics_from_input

      - name: Run pytest (check output files)
        run: pytest tests/

      # AWS CLI
      - name: Install AWS CLI
        run: |
          sudo apt-get update
          sudo apt-get install -y awscli

      - name: Verify file uploaded to AWS S3
        run: |
          aws s3 ls s3://$AWS_S3_BUCKET/metrics/ --region $AWS_REGION
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION }}
          AWS_S3_BUCKET: ${{ secrets.AWS_S3_BUCKET }}

      # GCP CLI
      - name: Install gcloud CLI
        run: |
          sudo apt-get update
          sudo apt-get install -y google-cloud-cli

      - name: Authenticate to GCP
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}

      - name: Verify file uploaded to GCS
        run: |
          gsutil ls gs://$GCS_BUCKET/metrics/
        env:
          GCS_BUCKET: ${{ secrets.GCS_BUCKET }}

      - name: Verify data loaded into BigQuery
        run: |
          bq query --use_legacy_sql=false \
          "SELECT * FROM \`${{ secrets.BQ_PROJECT }}.${{ secrets.BQ_DATASET }}.retail_metrics\` LIMIT 5"
