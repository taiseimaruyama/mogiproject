name: CI Pipeline

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  e2e-test:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set up Docker Compose
        run: |
          sudo apt-get update
          sudo apt-get install -y docker-compose

      - name: Start Airflow with Docker Compose
        run: |
          docker-compose up -d
          echo "⏳ Waiting for Airflow to start..."
          sleep 60

      - name: Initialize Airflow DB
        run: |
          docker-compose run --rm airflow-cli airflow db migrate

      - name: Wait for Airflow webserver to be ready
        run: |
          for i in {1..10}; do
            if docker ps --format '{{.Names}}' | grep -q "airflow-webserver"; then
              echo "✅ Airflow webserver is running"
              break
            fi
            echo "⏳ Waiting for webserver... ($i)"
            sleep 15
          done

      - name: Trigger DAG run
        run: |
          WEB_SERVER=$(docker ps --format "{{.Names}}" | grep airflow-webserver)
          docker exec $WEB_SERVER airflow dags trigger -d industry_metrics_from_input
          echo "⏳ Waiting for DAG execution..."
          sleep 120
          docker exec $WEB_SERVER airflow dags state industry_metrics_from_input $(date +%Y-%m-%d) --output json || true

      - name: Run pytest (check output files)
        run: |
          pip install pytest pandas
          pytest -v

      # =====================
      # AWS S3
      # =====================
      - name: Install AWS CLI
        run: |
          sudo apt-get update
          sudo apt-get install -y unzip curl
          curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
          unzip -q awscliv2.zip
          sudo ./aws/install --update

      - name: Verify file uploaded to AWS S3
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: ap-northeast-1
        run: |
          aws s3 ls s3://my-s3-bucket/metrics/ads_metrics.csv

      # =====================
      # GCP GCS + BigQuery
      # =====================
      - name: Install gcloud CLI
        run: |
          sudo apt-get update
          sudo apt-get install -y google-cloud-cli

      - name: Authenticate to GCP
        env:
          GOOGLE_APPLICATION_CREDENTIALS: ${{ github.workspace }}/gcp-key.json
        run: |
          echo '${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}' > $GOOGLE_APPLICATION_CREDENTIALS
          gcloud auth activate-service-account --key-file=$GOOGLE_APPLICATION_CREDENTIALS
          gcloud config set project striking-yen-470200-u3

      - name: Verify file uploaded to GCS
        run: |
          gsutil ls gs://my-gcs-bucket-2025-demo/metrics/retail_metrics.csv

      - name: Verify data loaded into BigQuery
        run: |
          bq query --use_legacy_sql=false \
          "SELECT COUNT(*) as cnt FROM \`striking-yen-470200-u3.sales_dataset.retail_metrics\`"
