name: CI

on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]

jobs:
  build:
    runs-on: ubuntu-latest
    env:
      AIRFLOW_FERNET_KEY: ${{ secrets.AIRFLOW_FERNET_KEY }}

    services:
      docker:
        image: docker:20.10-dind
        options: --privileged

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set up Docker Compose
        run: |
          sudo apt-get update
          sudo apt-get install -y docker-compose jq

      # Airflow ã®DBåˆæœŸåŒ– & Adminãƒ¦ãƒ¼ã‚¶ãƒ¼ä½œæˆ
      - name: Initialize Airflow DB
        run: docker compose run --rm airflow-init

      - name: Start Airflow with Docker Compose
        run: docker compose up -d

      - name: Wait for Airflow webserver to be healthy
        run: |
          for i in {1..24}; do
            if curl -s http://localhost:8080/health; then
              echo "âœ… Airflow is healthy"
              exit 0
            fi
            echo "â³ Still waiting... ($i)"
            sleep 5
          done
          echo "âŒ Airflow did not become healthy in time"
          exit 1

      - name: Trigger Airflow DAG
        run: |
          echo "ğŸš€ Triggering DAG industry_metrics_full_dag"
          docker compose exec -T airflow-webserver \
            airflow dags trigger industry_metrics_full_dag

      # === DAGã®å…¨ã‚¿ã‚¹ã‚¯è©³ç´°ãƒ­ã‚°ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§è¡¨ç¤º ===
      - name: Stream all Airflow task logs
        run: |
          echo "ğŸ“¡ Streaming all task logs for DAG industry_metrics_full_dag"

          # æœ€æ–°ã® DAG Run ID ã‚’å–å¾—
          RUN_ID=$(docker compose exec -T airflow-webserver \
            airflow dags list-runs -d industry_metrics_full_dag --no-backfill --output json \
            | jq -r '.[0].run_id')
          echo "ğŸ¯ Following logs for DAG Run: $RUN_ID"

          # ã‚¿ã‚¹ã‚¯ä¸€è¦§ã‚’å–å¾—
          TASKS=$(docker compose exec -T airflow-webserver \
            airflow tasks list industry_metrics_full_dag --output json | jq -r '.[].task_id')

          # å„ã‚¿ã‚¹ã‚¯ã®ãƒ­ã‚°ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ã‚¹ãƒˆãƒªãƒ¼ãƒ 
          for TASK in $TASKS; do
            echo "ğŸ“¡ Streaming logs for task: $TASK"
            docker compose exec -T airflow-webserver \
              airflow tasks logs industry_metrics_full_dag $TASK $RUN_ID --follow &
          done

          # å…¨ã‚¿ã‚¹ã‚¯ãŒçµ‚äº†ã™ã‚‹ã¾ã§å¾…æ©Ÿ
          wait

      - name: Test GCS connection
        run: |
          echo "Testing GCS connection..."
          # gsutil ls ã¨ã‹æ¥ç¶šç¢ºèªã‚³ãƒãƒ³ãƒ‰ã‚’å…¥ã‚Œã‚‹

      - name: Test S3 connection
        run: |
          echo "Testing S3 connection..."
          aws s3 ls s3://domoproject || true

      - name: Test BigQuery connection
        run: |
          echo "Testing BigQuery connection..."
          # bq query --use_legacy_sql=false "SELECT 1"

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-northeast-1

      - name: Upload output files to S3
        if: always()
        run: |
          echo "ğŸ“¤ Uploading files in ./output to S3..."
          ls -R output || echo "âš ï¸ output directory not found"
          if [ -d output ]; then
            aws s3 cp output/ s3://domoproject/ --recursive
          else
            echo "âš ï¸ No output directory found"
          fi
